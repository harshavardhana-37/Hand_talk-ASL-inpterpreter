ğŸ–ï¸ HandTalk: Real-Time ASL Interpreter using MediaPipe

HandTalk is a real-time American Sign Language (ASL) and gesture recognition system that uses MediaPipe, OpenCV, and deep learning to interpret hand gestures into spoken and written language. The project bridges the communication gap between sign language users and non-signers by translating gestures into text and speech â€” with multi-language support (English, Kannada, Hindi, Telugu).

â¸»

ğŸš€ Features
	â€¢	ğŸ¥ Real-time Hand Tracking using Googleâ€™s MediaPipe Hands
	â€¢	ğŸ§  Gesture Classification powered by a custom-trained neural network
	â€¢	ğŸ”¤ Automatic Translation of recognized gestures into multiple languages
	â€¢	ğŸ”Š Speech Output via text-to-speech (TTS) in the selected language
	â€¢	ğŸ–‹ï¸ Unicode Support for Indian languages (Kannada, Hindi, Telugu)
	â€¢	ğŸ’¾ Data Logging Mode â€” capture new gesture samples for model training
	â€¢	ğŸ§© Menu Interface for switching languages during live recognition

â¸»

ğŸ§° Tech Stack
	â€¢	Python 3
	â€¢	OpenCV â€“ for camera input and visual overlays
	â€¢	MediaPipe â€“ for hand landmark detection
	â€¢	TensorFlow / Keras â€“ for gesture classification models
	â€¢	PIL (Pillow) â€“ for rendering Unicode text
	â€¢	pyttsx3 / playsound â€“ for speech synthesis
	â€¢	NumPy, CSV, Argparse â€“ for utilities and dataset handling

â¸»

ğŸ“¦ Project Structure:
hand-gesture-recognition-mediapipe-main/
â”‚
â”œâ”€â”€ app.py                             # Main application
â”œâ”€â”€ model/                             # Pre-trained gesture classification models
â”œâ”€â”€ utils.py                           # FPS calculation and helper functions
â”œâ”€â”€ fonts/                             # Unicode fonts for language rendering
â”œâ”€â”€ voices/                            # Pre-recorded or TTS-generated audio clips
â””â”€â”€ requirements.txt                   # Dependencies
ğŸ§‘â€ğŸ’» How It Works
	1.	The webcam captures a live video feed.
	2.	MediaPipe detects and tracks the userâ€™s hand landmarks in real time.
	3.	A trained classifier predicts the performed gesture.
	4.	The recognized gesture is translated into the selected language using a predefined dictionary.
	5.	The system displays the text on screen and converts it to speech.

â¸»

ğŸŒ Supported Languages
	â€¢	English ğŸ‡ºğŸ‡¸
	â€¢	Kannada ğŸ‡®ğŸ‡³
	â€¢	Hindi ğŸ‡®ğŸ‡³
	â€¢	Telugu ğŸ‡®ğŸ‡³

â¸»

ğŸ—£ï¸ Demo

(Add a GIF or short video clip showing the real-time translation in action)

â¸»

ğŸ§© Future Improvements
	â€¢	Add support for continuous sign sentences
	â€¢	Improve dataset and model accuracy
	â€¢	Integrate camera calibration for better hand detection
	â€¢	Build a lightweight mobile version

â¸»

â¤ï¸ Authors

Harshavardhana & Team
B.E. Final Year Project â€” 2025
